# Crosslingual NLP

This repo supports various cross-lingual transfer learning & multilingual NLP models. It powers the following papars.

- Mahsa Yarmohammadi*, Shijie Wu*, Marc Marone, Haoran Xu, Seth Ebner, Guanghui Qin, Yunmo Chen, Jialiang Guo, Craig Harman, Kenton Murray, Aaron Steven White, Mark Dredze, and Benjamin Van Durme. [*Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction*](https://arxiv.org/abs/2109.06798). EMNLP. 2021. ([Experiments Detail](example/data-projection))
- Shijie Wu and Mark Dredze. [*Do Explicit Alignments Robustly Improve Multilingual Encoders?*](https://arxiv.org/abs/2010.02537) EMNLP. 2020. ([Experiments Detail](example/contrastive-alignment))
- Shijie Wu and Mark Dredze. [*Are All Languages Created Equal in Multilingual BERT?*](https://arxiv.org/abs/2005.09093) RepL4NLP. 2020. ([Experiments Detail](example/low-resource-in-mbert))
- Shijie Wu*, Alexis Conneau*, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. [*Emerging Cross-lingual Structure in Pretrained Language Models*](https://arxiv.org/abs/1911.01464). ACL. 2020. ([Experiments Detail](example/emerging-crossling-struct))
- Shijie Wu and Mark Dredze. [*Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT*](https://arxiv.org/abs/1904.09077). EMNLP. 2019. ([Experiments Detail](example/surprising-mbert))



## Miscellaneous

- Environment (conda): `environment.yml`
- Pre-commit check: `pre-commit run --all-files`

## License

MIT
